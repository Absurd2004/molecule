from __future__ import annotations

import argparse
import importlib
import sys
from pathlib import Path
from typing import Dict, Iterable, List, Optional, Tuple

import pandas as pd
import torch
import dgl
from torch.utils.data import DataLoader, Dataset

PACKAGE_DIR = Path(__file__).resolve().parent
PROJECT_ROOT = PACKAGE_DIR.parent.parent
if str(PROJECT_ROOT) not in sys.path:
	sys.path.insert(0, str(PROJECT_ROOT))


class CustomDataset(Dataset):
	def __init__(
		self,
		label_list: Iterable[torch.Tensor],
		graph_list: Iterable[dgl.DGLGraph],
		gap_list: Optional[Iterable[float]] = None,
	) -> None:
		self.labels = list(label_list)
		self.graphs = list(graph_list)
		self.gaps = list(gap_list) if gap_list is not None else None

	def __len__(self) -> int:
		return len(self.labels)

	def __getitem__(self, index: int):
		label = self.labels[index]
		if not torch.is_tensor(label):
			label_tensor = torch.tensor(label, dtype=torch.float32)
		else:
			label_tensor = label.to(torch.float32)

		graph = self.graphs[index]

		if self.gaps is not None:
			gap_val = self.gaps[index]
			gap_tensor = (
				gap_val
				if torch.is_tensor(gap_val)
				else torch.tensor(gap_val, dtype=torch.float32)
			)
			return label_tensor, graph, gap_tensor
		return label_tensor, graph


def collate_fn(batch):
	first = batch[0]
	if len(first) == 3:
		labels, graphs, gaps = zip(*batch)
		labels = torch.stack(labels)
		batched_graph = dgl.batch(graphs)
		gaps = torch.stack(
			[
				gap if torch.is_tensor(gap) else torch.tensor(gap, dtype=torch.float32)
				for gap in gaps
			]
		)
		return labels, batched_graph, gaps
	labels, graphs = zip(*batch)
	labels = torch.stack(labels)
	batched_graph = dgl.batch(graphs)
	return labels, batched_graph


def message_func(edges):
	return {"feat": edges.data["feat"]}


def reduce_func(nodes):
	mailbox = nodes.mailbox["feat"]
	if mailbox.ndim == 1:
		mailbox = mailbox.unsqueeze(0)
	agg_feats = mailbox.mean(dim=1)
	return {"agg_feats": agg_feats}


def update_node_features(graph: dgl.DGLGraph) -> dgl.DGLGraph:
	batched = graph.clone()
	batched.send_and_recv(batched.edges(), message_func, reduce_func)
	agg_feats = batched.ndata.pop("agg_feats")
	batched.ndata["feat"] = torch.cat((batched.ndata["feat"], agg_feats), dim=1)
	return batched


MODEL_CLASS_MAP: Dict[str, str] = {
	"ka_gnn": "KA_GNN",
	"ka_gnn_two": "KA_GNN_two",
	"mlp_sage": "MLPGNN",
	"mlp_sage_two": "MLPGNN_two",
	"kan_sage": "KANGNN",
	"kan_sage_two": "KANGNN_two",
}

DATASET_LABEL_DIM: Dict[str, int] = {
	"tox21": 12,
	"muv": 17,
	"sider": 27,
	"clintox": 2,
	"bace": 1,
	"bbbp": 1,
	"hiv": 1,
	"dft": 2,
}


def parse_args() -> argparse.Namespace:
	default_root = Path(__file__).resolve().parent
	parser = argparse.ArgumentParser(
		description=(
			"Export DFT train/valid/test labels alongside model predictions into CSV files."
		)
	)
	parser.add_argument(
		"--state-path",
		type=Path,
		default=default_root / "data" / "processed" / "dft.pth",
		help="Path to the cached dataset generated by main.py (default: data/processed/dft.pth).",
	)
	parser.add_argument(
		"--checkpoint",
		type=Path,\
		default= default_root/ "pretrained_models" / "best.pth",
		help=(
			"Path to a trained checkpoint (.pth). If omitted, the script searches for the most recent "
			"best.pth under runs/dft."
		),
	)
	parser.add_argument(
		"--output-dir",
		type=Path,
		default=default_root,
		help="Directory where the CSV files will be written (default: prediction_model root).",
	)
	parser.add_argument(
		"--batch-size",
		type=int,
		help="Optional override for inference batch size. Defaults to the cached state's batch size.",
	)
	return parser.parse_args()


def find_latest_checkpoint(root: Path) -> Optional[Path]:
	if not root.exists():
		return None
	best_files = list(root.glob("**/best.pth"))
	if not best_files:
		return None
	best_files.sort(key=lambda p: p.stat().st_mtime, reverse=True)
	return best_files[0]


def load_model(checkpoint_path: Path) -> Tuple[torch.nn.Module, torch.device, int, bool]:
	checkpoint = torch.load(checkpoint_path, map_location="cpu")
	args = checkpoint.get("args", {}) if isinstance(checkpoint, dict) else {}
	model_select = args.get("model_select", "ka_gnn")
	model_cls_name = MODEL_CLASS_MAP.get(model_select, "KA_GNN")
	model_module = importlib.import_module(
		"data_preparation.prediction_model.model.ka_gnn"
	)
	model_cls = getattr(model_module, model_cls_name)

	dataset_name = args.get("select_dataset", "dft")
	label_dim = int(args.get("label_dim", DATASET_LABEL_DIM.get(dataset_name, 1)))
	is_dft_dataset = dataset_name == "dft"

	grid_feat = int(args.get("grid_feat", 1))
	num_layers = int(args.get("num_layers", 4))
	hidden_feat = int(args.get("hidden_feat", 64))
	out_feat = int(args.get("out_feat", 32))
	pooling = args.get("pooling", "avg")
	in_feat = int(args.get("in_feat", 113))

	state_dict = (
		checkpoint.get("model_state_dict")
		if isinstance(checkpoint, dict)
		else checkpoint
	)

	def build_model(target_dim: int) -> torch.nn.Module:
		return model_cls(
			in_feat=in_feat,
			hidden_feat=hidden_feat,
			out_feat=out_feat,
			out=target_dim,
			grid_feat=grid_feat,
			num_layers=num_layers,
			pooling=pooling,
			use_bias=True,
		)

	target_dim = label_dim + (1 if is_dft_dataset else 0)
	model = build_model(target_dim)
	try:
		missing, unexpected = model.load_state_dict(state_dict, strict=False)
	except RuntimeError as exc:
		if is_dft_dataset:
			print(
				"[info] Checkpoint output dimension differs from expected 3; "
				"retrying with 2 outputs (no explicit gap head)."
			)
			target_dim = label_dim
			model = build_model(target_dim)
			missing, unexpected = model.load_state_dict(state_dict, strict=False)
		else:
			raise

	if missing:
		print(f"[warn] Missing keys in checkpoint: {missing}")
	if unexpected:
		print(f"[warn] Unexpected keys in checkpoint: {unexpected}")

	is_dft = is_dft_dataset and target_dim > label_dim

	device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
	model.to(device)
	model.eval()
	return model, device, label_dim, is_dft


def build_loader(
	labels: Iterable[torch.Tensor],
	graphs: Iterable[dgl.DGLGraph],
	gaps: Optional[Iterable[float]],
	batch_size: int,
) -> DataLoader:
	dataset = CustomDataset(labels, graphs, gaps if gaps is not None else None)
	return DataLoader(
		dataset,
		batch_size=batch_size,
		shuffle=False,
		drop_last=False,
		num_workers=0,
		collate_fn=collate_fn,
	)


def predict_split(
	name: str,
	loader: DataLoader,
	model: torch.nn.Module,
	device: torch.device,
	label_dim: int,
	is_dft: bool,
) -> pd.DataFrame:
	columns = [
		"s1_label",
		"t1_label",
		"gap_label",
		"s1_pred",
		"t1_pred",
		"gap_pred",
	]
	rows: List[Tuple[float, float, float, float, float, float]] = []
	if loader.dataset is None or len(loader.dataset) == 0:
		return pd.DataFrame(columns=columns)

	model.eval()
	with torch.no_grad():
		for batch in loader:
			if len(batch) == 3:
				labels, graphs, gaps = batch
				gaps = gaps.float().to(device)
			else:
				labels, graphs = batch
				gaps = None
			labels = labels.float().to(device)
			batched_graph = update_node_features(graphs)
			batched_graph = batched_graph.to(device)
			node_features = batched_graph.ndata["feat"].to(device)
			outputs = model(batched_graph, node_features)
			pred_core = outputs[:, :label_dim]
			pred_gap = (
				outputs[:, label_dim]
				if is_dft and outputs.size(1) > label_dim
				else pred_core[:, 0] - pred_core[:, 1]
			)

			labels_cpu = labels.cpu().numpy()
			pred_core_cpu = pred_core.cpu().numpy()
			pred_gap_cpu = pred_gap.cpu().numpy()
			if gaps is not None:
				gap_true = gaps.cpu().numpy()
			else:
				gap_true = labels_cpu[:, 0] - labels_cpu[:, 1]

			for idx in range(labels_cpu.shape[0]):
				rows.append(
					(
						float(labels_cpu[idx, 0]),
						float(labels_cpu[idx, 1]),
						float(gap_true[idx]),
						float(pred_core_cpu[idx, 0]),
						float(pred_core_cpu[idx, 1]),
						float(pred_gap_cpu[idx]),
					)
				)

	return pd.DataFrame(rows, columns=columns)


def main() -> None:
	args = parse_args()
	state_path = args.state_path
	if not state_path.exists():
		raise FileNotFoundError(f"Dataset state file not found: {state_path}")

	checkpoint_path = args.checkpoint
	if checkpoint_path is None:
		runs_root = Path(__file__).resolve().parent / "runs" / "dft"
		checkpoint_path = find_latest_checkpoint(runs_root)
		if checkpoint_path is None:
			raise FileNotFoundError(
				"Could not locate a checkpoint. Specify --checkpoint explicitly "
				"or ensure a 'best.pth' exists under runs/dft."
			)
	else:
		checkpoint_path = checkpoint_path.resolve()
	if not checkpoint_path.is_file():
		raise FileNotFoundError(f"Checkpoint file not found: {checkpoint_path}")

	print(f"Loading dataset from: {state_path}")
	print(f"Using checkpoint: {checkpoint_path}")
	state = torch.load(state_path, map_location="cpu")

	model, device, label_dim, is_dft = load_model(checkpoint_path)

	default_batch = int(state.get("batch_size", 128))
	batch_size = int(args.batch_size or default_batch)

	train_loader = build_loader(
		state["train_label"],
		state["train_graph_list"],
		state.get("train_gap"),
		batch_size,
	)
	valid_loader = build_loader(
		state["valid_label"],
		state["valid_graph_list"],
		state.get("valid_gap"),
		batch_size,
	)
	test_loader = build_loader(
		state["test_label"],
		state["test_graph_list"],
		state.get("test_gap"),
		batch_size,
	)

	datasets = {
		"train": train_loader,
		"valid": valid_loader,
		"test": test_loader,
	}

	output_dir = args.output_dir.resolve()
	output_dir.mkdir(parents=True, exist_ok=True)

	for split_name, loader in datasets.items():
		print(f"Predicting {split_name} split (batches: {len(loader)})")
		df = predict_split(split_name, loader, model, device, label_dim, is_dft)
		output_path = output_dir / f"{split_name}_dft_prediction.csv"
		df.to_csv(output_path, index=False)
		print(f"Saved {split_name} predictions to {output_path}")

	print("Done.")


if __name__ == "__main__":
	main()
